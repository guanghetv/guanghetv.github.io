---
layout: post  
title:  "MDP在洋葱（一）：用户课程规划与问题设定"  
category: tech  
published: true
author: "ronfe"
---

## 简介

洋葱数学目前已拥有较完整的初中数学课程学习体系。该体系的基本构成单位为知识点（topic）。每个知识点由1-2个视频及一组习题组成，并且具备一定的内容层次。具体来说，包括

* A类知识点：讲解基础概念
* B类知识点：讲解知识点内解题能力
* C-D类知识点：培养学生综合解题能力


如何更好地利用这些课程资源，为每个学生用户规划其最优的课程序列，就是本系列文章欲解决的主要问题。

本系列文章尝试通过设定一个常见的用户使用情景，使用增强学习（RL）的方法来解决课程规划的问题。具体而言，即已知用户之前在洋葱数学的知识点完成情况，输出一个其下一步应该完成的知识点，这个知识点能够使用户的预期收益最大化。举一个直觉上的类比，如果一个用户在某方面基础知识比较薄弱，那么就应该给TA指定一个A-B类的知识点；反过来，如果一个用户在某方面能力很强，应该让TA学习难度和思维能力要求更高的D类知识点。

有关增强学习和MDP，可参考 https://webdocs.cs.ualberta.ca/~sutton/book/bookdraft2016sep.pdf

## 问题情景

为了解决上面的问题，这个项目设定了一个使用场景：

假设用户可以每天使用洋葱数学来完成一定量的知识点学习/复习。并且每完成一个知识点，有且仅有“学习成功”和“学习失败”两种结果。
知识点的核心教学目标是用户可以正确解出练习部分最后一题（称为“目标题”）。如果用户目标题回答正确，该知识点学习成功。反之，如果用户练习失败，或目标题回答错误，则该知识点学习失败。

我们需要实现一个Agent，用户每完成一次知识点（无论结算为学习成功还是学习失败，都视为用户完成一次知识点学习），这个Agent就需要为其指定下一个要学习的知识点。

在这个情境的基础上，还有一些其他条件（下文将详述）：  

* 用户的时间是有限度的，假设用户可以学习 *D* 天，每天最多可学习 *k* 个知识点（但不同用户的D和k不同）。那么在 D+1 天就会进行一次考试。Agent的目标就是D+1天的考试中用户可以得到成绩的最大提升。
* 用户有焦虑属性，每天结束后，未掌握知识点数量越多，或者学习知识点失败，焦虑值均会不同程度地增加。如果焦虑值达到一定水平，用户每天学习的知识点容量就会下降。
* 用户有遗忘属性，每天结束后，用户所有知识点的掌握情况都有不同程度的下降。
* 在学习过程开始前，Agent可以通过信度效度可靠的前测或其他手段得到用户初始的知识点掌握程度。如果没有前测，则Agent可以默认用户没有接触过任何知识点的内容（即假设用户对所有知识点的初始掌握程度均为0）

### 能力模型

根据初中数学课程标准，数学能力可分为数学抽象、直观想象、数据分析、数学运算、逻辑推理、数学建模六个维度。初中数学教学内容均是围绕这六个能力维度而展开。体现到知识点上，即每个知识点都可以归类为某个或某几个能力维度。同样对每一个用户也需要估计其六个维度的能力水平。

### 知识点模型

一个知识点 *t* ，有两个核心属性：能力Set和掌握难度

* 能力Set *abilitySet(t)*：完成 *t* 所需具备的能力维度及其权重。当用户成功学习 *t* ， *t* 所对应的能力也会有增加（具体算法请见下文用户模型部分）。
* 掌握难度 *diff(t)*：用户掌握 *t* 的难度系数，用户能力水平不变的情况下，掌握难度越高，用户学习知识点成功的概率越低。

能力Set的属性由课程讲师人工设计并指定，每个知识点可以有多个不同的能力，但只有一个主要能力，主要能力的权重高于其他能力。在本例中，其他能力的权值相等。
掌握难度则由一系列参数决定。这些参数包括：

* 知识点类型（A-D，类型越高难度越大）
* 知识点所在位置（知识点所在上级课程单位（主题）中的相对位置，位置越靠后越难）
* 样本数据里知识点习题错误率：我们抽样了一段时间内用户在洋葱数学APP中看视频和做题的数据，并计算出样本数据中每个知识点题目的错误率，错误率越高则知识点越难。

理论上，我们可以根据这些经验数据训练一个线性/非线性的模型作为计算 *diff(t)* 的标准。但笔者基于MVP实现原则的考虑，在实现中暂时固定了算法，即线性模型，知识点类型权重0.4，所在位置权重0.2，习题错误率权重0.4。在知识点类型方面，固定A类为0.1，D类为0.4，依次等差增加。

例如，某知识点的这三项参数如下：

```
{
    "tid" : "5657c107d6b57c7550787654",
    "type" : "A",
    "incorrect" : 0.319067884884378,
    "abilitySet" : {
        "logic" : 0.625,
        "imagination" : 0.375
    },
    "position" : 0.333333333333333
}
```

其中，`tid`为知识点的ID，`type`为知识点的类型，`incorrect`为知识点的样本错误率（+0.1平滑），`position`为知识点的相对位置（+1平滑）。而`abilitySet`为该知识点的能力Set及权重。

根据前文的算法，该知识点的掌握难度为 0.1 （A类知识点权重） + 0.319 * 0.4（错误率） + 0.333 * 0.2 （相对位置） = 0.294

再以另一个难度更高的知识点为例：

```
{
    "tid" : "5791c74694292f1d63985ede",
    "type" : "D",
    "incorrect" : 0.354838709677419,
    "abilitySet" : {
        "imagination" : 0.455,
        "logic" : 0.545
    },
    "position" : 0.75
}
```

该知识点的掌握难度为 0.4 （D类知识点权重） + 0.355 * 0.4 （错误率） + 0.75 * 0.2 （相对位置） = 0.692

此外，后期还有一些维度待评估标准完善后可以加入到参数中，可以更加全面地反映知识点的掌握难度。例如：

* 练习题做题时间
* 视频播放时长比 （人均有效视频播放时长/总时长）

### 用户模型与知识点学习过程

用户模型主要包含用户能力和用户-知识点掌握矩阵。

* 用户能力 *userAbility(u)* 为一个用户 *u* 对六个能力维度的能力值，每个能力值范围均在 [0,1] 。每一个用户都有六个维度的能力值，因此为 *R^6*。
* 用户-知识点掌握矩阵 *Learned(u, t)* 为用户 *u* 对一个知识点 *t* 的掌握程度，范围在 [0,1]。当 *u* 学习 *t* 成功时，*Learned(u, t)* = 1。此外，*Learned(u, t)* 随着时间的推移而下降（具体下降计算见下）。

我们将 *u* 单次学习 *t* 的行为记为 *A(u, t)*，值为True(学习成功) 或 False（学习失败），则*A(u, t)* = True 的期望与 *userAbility(u)* 中 *abilitySet(t)* 的值（记为 *ability(u, t)*），以及 *diff(t)* 相关。具体为：

*E(A(u, t) = True*|*u, t) = (1 - diff(t)) * ability(u, t)* 

由于用户学习知识点只可结算为学习成功或学习失败二者之一，故 *E(A(u, t) = False) = 1 - E(A(u, t) = True)*

根据上文的设定，如果 *A(u, t)* = True，那么 *Learned(u, t)* = 1。

而用户某个能力维度*a* 的能力值计算则与用户在 *a* 所属所有知识点的掌握程度相关。假设一个用户在 *a* 所属所有知识点的掌握度均为 1， 则该用户对 *a* 的能力值也为1。用户 *u* 对 *a* 的能力值计算为：

*userAbility(u, a) = sum((Learned(u, t) * diff(t))) / sum(diff(t))* ， *t ∈ a*

实际上，用户能力值是以知识点掌握难度作为权重的加权平均数。这样处理的一个好处就是能力值本身在于体现用户对解决该维度问题的情况，知识点难度越高，自然掌握知识点后对能力的反馈就越高。

### 时间的影响与焦虑值

上文曾经提到，“时间”在这个问题中也起到了重要作用。时间的效应体现在两个层面上：

* 首先，每天过去以后（相当于用户完成了 *k_d* 个 *A(u, * )* 的学习任务，*k_d* 为用户在当前日 *d* 下的学习容量），*Learned(u, * )* 会出现不同程度的下降。这样设定也是为了鼓励Agent适时安排复习。
* 其次，每天过去以后，由于距离考试日近了一天，用户的焦虑值 *Anxiety(u)* 会上升，上升幅度为
  *Sum(Learned(u, t) < λ) / Len(T) * d_e/D, t∈T*  
  其中，*λ* 是一个掌握知识点的认知阈值，在实现中为0.6；*T* 为知识点总集；*d_e* 为用户已经经历的天数。 也就是说，每天焦虑值增长的情况与未掌握知识点的比重和已经过天数的比重相关。未掌握知识点数量越多，已经过天数越多，单日焦虑值上升越快

此外，当 *A(u, t) = False* 时， *Learned(u, t)* 不会减少，但 *Anxiety(u)* 会增加。以模拟学习失败时用户的挫败感。

*Anxiety*的作用主要体现在对用户单日学习容量 *k* 的影响上。这种影响是累进的，即有一组焦虑梯度矩阵。当焦虑值超过该矩阵的某个梯级时，用户积极性减弱，单日学习容量 *k* 就会减少一个，直至 *k_d = 0*，用户流失。

### 考试与Action rewards设定

我们可以这样模拟作为目标的考试过程：

* 首先，确定考试总量 *ME*，
* 在 *T*中根据知识点`type`分层随机抽取总量为*ME*的知识点子集 *TE*。分层比例为 A类0.3 - B类0.5 - C/D类0.2
* 遍历 *TE*中每个知识点 *t*:
  - 生成一个[0,1) 的随机数 *r*
  - 如果 *r <= Learned(u, t)* ，则正确题目数 *TC* + 1
* 测试成绩 *R = TC / Len(TE)*

可以看到，测试结果只会受到用户对知识点掌握程度的影响。因此用户每完成一个 *A(u, t)*，其 Reward为 *δE = LearnedAfterAction(u, t) - LearnedBeforeAction(u, t)*。但由于时间因素的影响，每日结束后掌握度会降低，因此还需要将降低的掌握度平摊到每次Action中。

## 总结

本文构建了一个比较典型的用户使用洋葱数学进行学习的场景，并描述了Agent要解决的问题。在这个问题中，提高用户在考试里的成绩是最终目的。而决定考试成绩的因素主要是用户对考试出现知识点的掌握度。用户提高一个知识点掌握度的途径则是学习成功该知识点，而知识点学习成功取决于知识点所属能力维度Set及用户在这些维度上的能力值，以及知识点本身的掌握难度。

此外，时间因素和焦虑的引入，更加逼真地模拟了用户的真实使用场景，也为Agent规划复习内容提供了可能。

目前我们的自适应课程体系研发还处在实验室阶段，因此在这个试验性的项目中，遵循了最小可用产品原则，在一些细节上采用了最为省力或“粗暴”的方式解决。在整个系统初步完成后我们还可以针对这些方面进行进一步调优。

该项目代码开源，Github repo: https://github.com/guanghetv/yangcong-mdp ，但在项目初步完成前，由于数据结构经常变动，源数据暂无法开放。
